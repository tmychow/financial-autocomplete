{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Toggle ChatML for ART calls (Qwen)\n",
    "# import os\n",
    "\n",
    "# # Set to True to enable ChatML formatting in the ART call path, False to disable\n",
    "# ART_USE_CHATML = True\n",
    "# os.environ[\"ART_USE_CHATML\"] = \"1\" if ART_USE_CHATML else \"0\"\n",
    "# print(\"ART_USE_CHATML =\", os.environ[\"ART_USE_CHATML\"]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finance Autocomplete RL Training\n",
    "\n",
    "This notebook trains a small language model to perform financial data autocomplete using reinforcement learning with tool calls.\n",
    "\n",
    "The model learns to:\n",
    "1. Use financial tools (search, calculate) to retrieve and compute data\n",
    "2. Complete text with accurate financial information\n",
    "3. Determine when no completion is needed\n",
    "\n",
    "Rewards combine LLM-as-judge correctness with tool-use bonuses (used search, required lookup coverage, and per-dimension ticker/metric/period correctness).\n",
    "\n",
    "Training uses the ART (Adaptive Reinforcement Training) framework with PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "import psutil\n",
    "\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!uv pip install openpipe-art==0.3.11.post3 \"gql<4\" --prerelease allow --no-cache-dir\n",
    "!uv pip install openpipe aiosqlite httpx tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository to get the Python modules\n",
    "!git clone https://github.com/tmychow/financial-autocomplete.git /content/financial-autocomplete 2>/dev/null || true\n",
    "import sys\n",
    "sys.path.insert(0, '/content/financial-autocomplete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Required: OpenAI API key for LLM-as-judge evaluation\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")  # @param {type:\"string\"}\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "else:\n",
    "    print(\"Warning: No OpenAI API key provided. Judge evaluation will fall back to simple string matching.\")\n",
    "\n",
    "# Required: Tiingo API for financial data\n",
    "TIINGO_API_KEY = os.getenv(\"TIINGO_API_KEY\", \"\")  # @param {type:\"string\"}\n",
    "if TIINGO_API_KEY:\n",
    "    os.environ[\"TIINGO_API_KEY\"] = TIINGO_API_KEY\n",
    "else:\n",
    "    raise ValueError(\"TIINGO_API_KEY is required. Get a free key at https://api.tiingo.com\")\n",
    "\n",
    "# Optional: Weights & Biases for metrics tracking\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\", \"\")  # @param {type:\"string\"}\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_STEPS = 80  # @param {type:\"integer\"}\n",
    "NUM_CASES_PER_STEP = 20  # @param {type:\"integer\"}\n",
    "NUM_ROLLOUTS_PER_CASE = 5  # @param {type:\"integer\"}\n",
    "NUM_VALIDATION_CASES = 50  # @param {type:\"integer\"}\n",
    "VALIDATION_FREQUENCY = 5  # @param {type:\"integer\"}\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"finance_autocomplete_model_v14\"  # @param {type:\"string\"}\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # @param {type:\"string\"}\n",
    "VAL_BENCHMARK_MODEL = \"gpt-4.1-mini\"  # @param {type:\"string\"}\n",
    "\n",
    "# Define baseline models to compare\n",
    "baseline_models = [\n",
    "    (\"gpt-4.1-mini\", \"gpt-4.1-mini\"),\n",
    "]\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 5e-5  # @param {type:\"number\"}\n",
    "BETA = 0.0  # @param {type:\"number\"}\n",
    "\n",
    "# Reward configuration\n",
    "USE_JUDGE = OPENAI_API_KEY != \"\"  # Use LLM judge if API key provided\n",
    "JUDGE_MODEL = os.getenv(\"JUDGE_MODEL\", \"gpt-4.1\")  # @param {type:\"string\"}\n",
    "os.environ[\"JUDGE_MODEL\"] = JUDGE_MODEL\n",
    "\n",
    "# Logging configuration\n",
    "import os\n",
    "import time\n",
    "LOG_DIR = \"./logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "RUN_TS = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "TRAIN_LOG_PATH = os.path.join(LOG_DIR, f\"train_{RUN_TS}.jsonl\")\n",
    "VAL_LOG_PATH = os.path.join(LOG_DIR, f\"validation_{RUN_TS}.jsonl\")\n",
    "print(f\"Logs will be written to:\\n  {TRAIN_LOG_PATH}\\n  {VAL_LOG_PATH}\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Training steps: {NUM_STEPS}\")\n",
    "print(f\"  Cases per step: {NUM_CASES_PER_STEP}\")\n",
    "print(f\"  Rollouts per case: {NUM_ROLLOUTS_PER_CASE}\")\n",
    "print(f\"  Total trajectories per step: {NUM_CASES_PER_STEP * NUM_ROLLOUTS_PER_CASE}\")\n",
    "print(f\"  Using LLM judge: {USE_JUDGE}\")\n",
    "print(f\"  Judge model: {JUDGE_MODEL}\")\n",
    "print(f\"  Using Tiingo data: {bool(TIINGO_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRAIN_LOG_PATH\"] = TRAIN_LOG_PATH\n",
    "os.environ[\"VAL_LOG_PATH\"] = VAL_LOG_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "import art\n",
    "\n",
    "# Import our finance modules\n",
    "from database import setup_database, get_tickers_with_data\n",
    "from synthetic import generate_cases, generate_training_data\n",
    "from environment import FinancialEnvironment\n",
    "from agent import AutocompleteAgent\n",
    "from rewards import calculate_reward\n",
    "from rollout import (\n",
    "    run_single_rollout,\n",
    "    conduct_rollouts,\n",
    "    run_validation\n",
    ")\n",
    "\n",
    "# Reward weighting (optional overrides)\n",
    "os.environ[\"W_USED_SEARCH\"] = os.getenv(\"W_USED_SEARCH\", \"0.1\")\n",
    "os.environ[\"W_TICKER\"] = os.getenv(\"W_TICKER\", \"0.0667\")\n",
    "os.environ[\"W_METRIC\"] = os.getenv(\"W_METRIC\", \"0.0667\")\n",
    "os.environ[\"W_PERIOD\"] = os.getenv(\"W_PERIOD\", \"0.0666\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the financial database with Tiingo data\n",
    "await setup_database()  # Uses TIINGO_API_KEY from environment\n",
    "\n",
    "# Verify data\n",
    "tickers = await get_tickers_with_data()\n",
    "print(f\"\\nDatabase contains data for {len(tickers)} tickers: {tickers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample test cases to verify everything works\n",
    "sample_cases = await generate_cases(5)\n",
    "print(\"Sample test cases:\")\n",
    "for i, case in enumerate(sample_cases, 1):\n",
    "    print(f\"\\n{i}. Input: {case['input']}\")\n",
    "    print(f\"   Expected: {case['ground_truth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainable model\n",
    "from art.local import LocalBackend\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=MODEL_NAME,\n",
    "    project=\"finance-autocomplete-rl\",\n",
    "    base_model=BASE_MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Create benchmark model for validation\n",
    "benchmark_model = art.Model(\n",
    "    name=VAL_BENCHMARK_MODEL,\n",
    "    project=\"finance-autocomplete-rl\",\n",
    "    inference_model_name=VAL_BENCHMARK_MODEL,\n",
    "    inference_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    inference_base_url=\"https://api.openai.com/v1\",\n",
    ")\n",
    "\n",
    "# Setup backend\n",
    "backend = LocalBackend(path=\"./.art\")\n",
    "\n",
    "# Register models\n",
    "await model.register(backend)\n",
    "await benchmark_model.register(backend)\n",
    "\n",
    "print(f\"Models registered:\")\n",
    "print(f\"  Training model: {MODEL_NAME}\")\n",
    "print(f\"  Benchmark model: {VAL_BENCHMARK_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Rollout\n",
    "\n",
    "Before training, let's test a single rollout to ensure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "\n",
    "def _iter_turns(traj, include_system=False):\n",
    "    # Support either a list or an object with .messages_and_choices\n",
    "    items = getattr(traj, \"messages_and_choices\", traj)\n",
    "\n",
    "    for it in items:\n",
    "        # Plain dict like {\"role\": \"...\", \"content\": \"...\"}\n",
    "        if isinstance(it, dict) and \"role\" in it and \"content\" in it:\n",
    "            role, content = it[\"role\"], it.get(\"content\", \"\")\n",
    "        # OpenAI-style Choice with .message.role / .message.content\n",
    "        elif hasattr(it, \"message\") and hasattr(it.message, \"content\"):\n",
    "            role = getattr(it.message, \"role\", \"assistant\")\n",
    "            content = it.message.content or \"\"\n",
    "        # Fallback\n",
    "        else:\n",
    "            role, content = \"other\", str(it)\n",
    "\n",
    "        if (role == \"system\" and not include_system):\n",
    "            continue\n",
    "        yield role, content\n",
    "\n",
    "def print_conversation(traj, width=100, truncate=0):\n",
    "    for i, (role, content) in enumerate(_iter_turns(traj), 1):\n",
    "        text = content.strip()\n",
    "        if truncate and len(text) > truncate:\n",
    "            text = text[:truncate].rstrip() + \" …[truncated]\"\n",
    "        print(f\"{i:02d} | {role.upper()}:\")\n",
    "        # Keep existing newlines, wrap each line for readability\n",
    "        for line in text.splitlines() or [\"\"]:\n",
    "            print(fill(line, width=width, subsequent_indent=\"    \"))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a single rollout\n",
    "test_case = sample_cases[0]\n",
    "print(f\"Testing rollout with: {test_case['input']}\")\n",
    "print(f\"Expected: {test_case['ground_truth']}\")\n",
    "\n",
    "result = await run_single_rollout(\n",
    "    model=model,\n",
    "    test_case=test_case,\n",
    "    rollout_id=0,\n",
    "    step=0,\n",
    "    use_judge=USE_JUDGE\n",
    ")\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(f\"\\nPrediction: {result['completion']}\")\n",
    "    print(f\"Reward: {result['reward_info']['total_reward']:.3f}\")\n",
    "    print(f\"  - Correctness: {result['reward_info']['correctness_score']:.3f}\")\n",
    "    print(f\"Tool calls: {result['episode_info']['tool_calls_count']}\")\n",
    "    traj = result['trajectory']\n",
    "    print_conversation(traj)\n",
    "else:\n",
    "    print(f\"Error: {result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for step in range(await model.get_step(), NUM_STEPS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Step {step}/{NUM_STEPS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Determine curriculum stage from step (override with CURRICULUM_STAGE if set)\n",
    "    curriculum_stage = 0 if step < 20 else (1 if step < 40 else (2 if step < 60 else 3))\n",
    "    print(f\"Curriculum stage: {curriculum_stage}\")\n",
    "\n",
    "    # Generate training cases for this step using curriculum\n",
    "    train_cases = await generate_cases(NUM_CASES_PER_STEP, curriculum_stage=curriculum_stage)\n",
    "\n",
    "    # Conduct rollouts\n",
    "    trajectory_groups = await conduct_rollouts(\n",
    "        model=model,\n",
    "        test_cases=train_cases,\n",
    "        num_rollouts_per_case=NUM_ROLLOUTS_PER_CASE,\n",
    "        step=step,\n",
    "        use_judge=USE_JUDGE,\n",
    "        judge_model=JUDGE_MODEL,\n",
    "        log_path=TRAIN_LOG_PATH,\n",
    "    )\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    if trajectory_groups:\n",
    "        total_trajectories = sum(len(tg.trajectories) for tg in trajectory_groups)\n",
    "        avg_reward = sum(t.reward for tg in trajectory_groups for t in tg.trajectories) / total_trajectories\n",
    "        avg_correct = sum(t.metrics.get(\"is_correct\", 0) for tg in trajectory_groups for t in tg.trajectories) / total_trajectories\n",
    "        \n",
    "        print(f\"Training metrics:\")\n",
    "        print(f\"  Trajectories: {total_trajectories}\")\n",
    "        print(f\"  Avg reward: {avg_reward:.3f}\")\n",
    "        print(f\"  Accuracy: {avg_correct:.1%}\")\n",
    "    \n",
    "    # Run validation periodically\n",
    "    if step % VALIDATION_FREQUENCY == 0 and USE_JUDGE:\n",
    "        print(f\"\\nRunning validation...\")\n",
    "        val_trajectories = await run_validation(\n",
    "            my_model=model,\n",
    "            benchmark_model=benchmark_model,\n",
    "            num_validation_cases=NUM_VALIDATION_CASES,\n",
    "            step=step,\n",
    "            use_judge=USE_JUDGE,\n",
    "            judge_model=JUDGE_MODEL,\n",
    "            log_path=VAL_LOG_PATH,\n",
    "        )\n",
    "        \n",
    "        if val_trajectories:\n",
    "            win_rate = sum(t.reward for t in val_trajectories) / len(val_trajectories)\n",
    "            print(f\"Validation win rate vs {VAL_BENCHMARK_MODEL}: {win_rate:.1%}\")\n",
    "            \n",
    "            # Log validation trajectories\n",
    "            await model.log(val_trajectories)\n",
    "    \n",
    "    # Train the model\n",
    "    if trajectory_groups:\n",
    "        await model.train(\n",
    "            trajectory_groups=trajectory_groups,\n",
    "            config=art.TrainConfig(\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                beta=BETA\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Clean up old checkpoints\n",
    "        await model.delete_checkpoints()\n",
    "        \n",
    "        print(f\"✓ Step {step} completed\")\n",
    "    else:\n",
    "        print(f\"⚠ No trajectories generated for step {step}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training completed!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Against Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare final model against various baselines\n",
    "if USE_JUDGE:\n",
    "    print(\"Benchmarking against baseline models...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_id, model_name in baseline_models:\n",
    "        baseline_model = art.Model(\n",
    "            name=model_id,\n",
    "            project=\"finance-autocomplete-rl\",\n",
    "            inference_model_name=model_name,\n",
    "            inference_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            inference_base_url=\"https://api.openai.com/v1\",\n",
    "        )\n",
    "        \n",
    "        await baseline_model.register(backend)\n",
    "        \n",
    "        # Run evaluation\n",
    "        val_trajectories = await run_validation(\n",
    "            my_model=baseline_model,\n",
    "            benchmark_model=benchmark_model,\n",
    "            num_validation_cases=NUM_VALIDATION_CASES,\n",
    "            step=0,\n",
    "            use_judge=USE_JUDGE,\n",
    "            judge_model=JUDGE_MODEL,\n",
    "        )\n",
    "        \n",
    "        if val_trajectories:\n",
    "            win_rate = sum(t.reward for t in val_trajectories) / len(val_trajectories)\n",
    "            avg_reward = sum(t.metrics.get(\"my_reward\", 0) for t in val_trajectories) / len(val_trajectories)\n",
    "            results[model_id] = {\"win_rate\": win_rate, \"avg_reward\": avg_reward}\n",
    "            \n",
    "            # Log for comparison\n",
    "            await baseline_model.log(val_trajectories)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(f\"{'Model':<20} {'Win Rate':<15} {'Avg Reward':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for model_id, metrics in results.items():\n",
    "        print(f\"{model_id:<20} {metrics['win_rate']:.1%}{'':.<8} {metrics['avg_reward']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize training progress\n",
    "from art.utils.benchmarking.load_trajectories import load_trajectories\n",
    "from art.utils.benchmarking.charts import training_progress_chart\n",
    "from art.utils.benchmarking.types import BenchmarkModelKey\n",
    "\n",
    "# Load trajectories\n",
    "df = await load_trajectories(\n",
    "    project_name=\"finance-autocomplete-rl\",\n",
    "    models=[MODEL_NAME],\n",
    "    art_path=\"./.art\",\n",
    ")\n",
    "\n",
    "# Plot win rate over time\n",
    "if not df.empty and USE_JUDGE:\n",
    "    models_to_plot = [\n",
    "        BenchmarkModelKey(MODEL_NAME, MODEL_NAME, \"val\"),\n",
    "    ]\n",
    "    \n",
    "    # Add baseline models if they were evaluated\n",
    "    if 'results' in locals():\n",
    "        for model_id, _ in baseline_models:\n",
    "            if model_id in results:\n",
    "                models_to_plot.append(BenchmarkModelKey(model_id, model_id.upper(), \"val\"))\n",
    "    \n",
    "    chart = training_progress_chart(\n",
    "        df,\n",
    "        \"win_rate\",\n",
    "        models=models_to_plot,\n",
    "        title=\"Win Rate vs Benchmark Over Time\",\n",
    "        y_label=\"Win Rate\",\n",
    "    )\n",
    "    \n",
    "    chart.savefig(\"finance_autocomplete_training.png\")\n",
    "    print(\"Training chart saved to finance_autocomplete_training.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final trained model on some examples\n",
    "print(\"Testing final model on sample inputs...\\n\")\n",
    "\n",
    "test_inputs = [\n",
    "    \"Apple's revenue in 2023 was \",\n",
    "    \"The gross margin for Microsoft in 2023Q4 was \",\n",
    "    \"Google's market cap in 2023Q4 was \",\n",
    "    \"The debt to equity ratio for Apple in 2023FY was \",\n",
    "    \"The CFO mentioned that \",\n",
    "]\n",
    "\n",
    "agent = AutocompleteAgent(model=model)\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    print(f\"Input: {input_text}\")\n",
    "    \n",
    "    completion, tool_calls, info = await agent.get_completion(input_text)\n",
    "    \n",
    "    print(f\"Completion: {completion}\")\n",
    "    print(f\"Tool calls: {info['tool_calls_count']}\")\n",
    "    \n",
    "    if tool_calls and info['tool_calls_count'] <= 10:\n",
    "        print(\"Tool sequence:\")\n",
    "        for tc in tool_calls[:10]:  # Show first 10 tools\n",
    "            args_str = \", \".join(f\"{k}={v}\" for k, v in tc.get('arguments', {}).items())\n",
    "            print(f\"  - {tc['tool']}({args_str})\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training configuration and results\n",
    "import json\n",
    "\n",
    "training_info = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"base_model\": BASE_MODEL_NAME,\n",
    "    \"num_steps\": NUM_STEPS,\n",
    "    \"cases_per_step\": NUM_CASES_PER_STEP,\n",
    "    \"rollouts_per_case\": NUM_ROLLOUTS_PER_CASE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"beta\": BETA,\n",
    "    \"use_judge\": USE_JUDGE,\n",
    "    \"judge_model\": JUDGE_MODEL,\n",
    "}\n",
    "\n",
    "with open(\"training_info.json\", \"w\") as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"Training information saved to training_info.json\")\n",
    "print(f\"\\nModel checkpoint available at: ./.art/{MODEL_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
